# Cache

In mezzo tra memoria e CPU. 

L'indirizzo prima di passare alla memoria viene preso dalla Cache che controlla se ha a disposizione il dato interessato. C'è una "tabella":

Indirizzo | Dato
0xAddr      XYZ

Se il dato non è nella tabella vado a cercarlo in memoria e lo metto nella cache da qualche parte.

Si potrebbe parlare anche di memoria virtuale, si usa il disco come memoria per alcuni dati, quidi si fa MemVirt->Mem->Cache->CPU. 

Ovviamente a seconda di quanta strada c'è da percorrere per arrivare alla CPU cambia il tempo  di accesso. Si passa dai nanosecondo ai millisecondi.


Se volessi scrivere in memoria? Volendo potrei cambiare la cache e basta (scrivendo in memoria una volta che quella cella nella cache viene "liberata", write-back), ma questo creerebbe inconsistenza in situazioni multiCPU. Allora la cosa che posso fare è scrivere in memoria "in background" (metodo write-through), dico alla cache "voglio scrivere X", aggiorno subito la cache ed aggiorno la memoria in background (non bloccante).

Hit-rate: quante volte succede che l'informazione che mi serve è in cache, e quante volte è in memoria centrale?

Esempio

Hit-rate: 99%
cache 1ns
mem 100ns (miss penality)

su 100 accessi = 100ns (l'accesso va fatto sempre) + 100ns (mem*1)  = 200

tempo medio = 200/100 = 2ns


L'accesso alla ram non è equiprobabile, non è troppo vero che in un momento X devo accedere agli indirizzi da 0 a 2^32. La cosa migliore è prendere oltre che alla cella interessata prendo anche quelle vicine, perchè è MOLTO probabile che le prossime operazioni richiederanno le celle adiacenti. 


The memory hierarchy wouldn't be very effective if two facts about programs weren't true. Programs exhibit spatial locality and temporal locality.

* spatial locality Spatial locality says that if you access memory address x, you are like to access memory address x plus or minus Delta where Delta is a small number. That is, you are likely to access addresses very near x in the near future.
Do programs exhibit spatial locality? Yes, they do. For example, when you accessing data from arrays, you are accessing memory locations that are contiguous. Also, instructions usually run sequentially (with the occasionaly branch or jump). Since instructions are contiguous in memory, they exhibit spatial locality. When you run one instruction, you're likely to run the next one too. That's the nature of running programs.
* temporal locality Temporal locality says that if you access memory address x at time t, you are like to access memory address x at time t +/- Delta. That is, you are likely to access the same address sometime soon.
This happens when you run code in a loop. You access the same instructions over and over. If you process an array in a loop (say, bubble sorting), then you access array elments over and over again too.

Cercare il direct mapping (cosa è?)

A direct-mapped cache scheme makes picking the slot very simple. It treats the slot as a large array, and the index of the array is picked from bits of the address (which is why we need the number of slots to be a power of 2---otherwise we can't select bits from the address)
The scheme can suffer from many addresses "colliding" to the same slot, thus causing the cache line to be repeatedly evicted, even though there may be empty slots that aren't being used, or being used with less frequency.