\section{Lezione 18 - Feature selection e Kernel learning}\label{lezione-18---feature-selection-e-kernel-learning}

Gli attributi o variabili dovrebbero essere utilizzati solo se veramente
utili (rilevanti) per la classificazione/predizione.

Avere meno attributi porta modelli di classificazione-predizione più
compatti e che hanno bisogno di un numero minore di esempi di
apprendimento per ottenere dei buoni risultati. Infatti, alcuni
attributi possono introdurre del rumore sui dati.

Inoltre, modelli che usano pochi attributi sono più semplici da
comprendere per un umano e sono più facilmente rappresentabili.

\subsection{Feature selection ed extraction}\label{feature-selection-ed-extraction}

\textbf{Feature selection}: viene selezionato un sotto insieme
``migliore'' degli attributi tra quelli originali. Ad esempio possono
essere scartate delle feature ridondanti o non rilevanti.

In questo modo si ottiene un interpretabilità migliore del modello
predittivo, pertanto questo approccio è preferibile dove
l'interpretabilità è più importante dell'accuratezza.

\textbf{Feature extraction}: si derivano nuovi attributi da quelli
originali, per esempio, nuove features vengono ottenute come
combinazione di attributi.

In questo modo si ottengono feature più discriminative che portano ad
avere un'accuratezza migliore, pertanto questo approccio è preferibile
per applicazioni dove l'accuratezza è più importante
dell'interpretabilità.

\subsubsection{Feature selection}\label{feature-selection}

Ci sono tre famiglie principali di metodi:

\begin{itemize}
\item
  \textbf{Filter methods}: considerano caratteristiche generali del
  training set, andando a pre-processare i dati indipendentiste
  dall'algoritmo di apprendimento. Viene calcolato uno score per le
  varie feature e vengono tenute solamente quelle migliori, senza tenere
  conto del supervisiore. Ad esempio questo metodo va a scartare le
  informazioni relative al nome di una persona.
\item
  \textbf{Wrapper methods}: la selezione delle feature viene fatta sulla
  base della loro capacità predittive, tipicamente utilizzando un
  insieme di hold-out. Può essere utilizzato un'approccio
  \textbf{backward}, partendo prima con tutte le feature possibili,
  dopodiché per ognuna delle feature si prova a toglierla e si controlla
  quanto migliora lo score. Si vanno via via a rimuovere in modo greedy
  le feature che portano ad un miglioramento dello score. L'approccio
  \textbf{forward} parte da un numero minimo di feature e va ad
  aggiungere feature in modo da aumentare lo score.
\item
  \textbf{Embedded methods}: la scelta delle feature viene inserita nel
  problema di minimizzazione, come nel metodo \textbf{LASSO}. Se
  \emph{w} è il vettore dei pesi che si va ad apprendere, si aggiunge al
  problema la massimizzazione della norma zero del vettore \emph{w}. Nel
  lato pratico si usa l'approssimazione ottenuta massimizzando la norma
  uno del vettore.
\end{itemize}

\subsubsection{Feature extraction}\label{feature-extraction}

Il metodo più importante si chiama \textbf{Principal Component Analysis}
(PCA) e consiste nella estrazione di un insieme di features non
correlate linearmente (componenti principali).

Ovvero si mappano tutti gli esempi in poche dimensioni, queste
dimensioni sono quelle che rappresentano meglio i dati del problema e
allo stesso tempo diminuiscono il rumore sui dati.

Il numero di componenti principali è solitamente molto inferiore al
numero di features originali.

\subsubsection{Applicazioni della Feature Selection}\label{applicazioni-della-feature-selection}

\begin{itemize}
\item
  \textbf{Biologia computazionale}: in quanto si hanno pochi esempi e
  migliaia di features. Ad esempio si vuole sapere, dato il paziente si
  vuole sapere se una cura può essere efficace o meno.
\item
  \textbf{Riconoscimento di facce}: per determinare quali sono le
  feature importanti per il riconoscimento della faccia.
\item
  \textbf{Ambito medico}: generalmente le variabili sono dei risultati
  degli esami medici, pertanto si cerca di minimizzare le variabili per
  ridurre i costi.
\item
  \textbf{Ambito finanziario}: moltissimi fattori possono influenzare un
  titolo in borsa. Si cerca quindi di ridurre questo numero per rendere
  più semplice il modello risultante.
\item
  \textbf{Classificazione dei testi}: ad ogni termine è associata una
  feature, riducendo questo numero si velocizza l'apprendimento.
\end{itemize}

\subsection{Kernel Learning}\label{kernel-learning}

L'idea è quella di apprendere la funzione o la matrice kernel:

\begin{itemize}
\item
  Metodi parametrici per il kernel learning
\item
  Transductive feature extraction con kernel non lineari
\item
  Spectral kernel learning
\item
  Multiple kernel learning
\end{itemize}

\textbf{Matrice kernel}: matrice che per ogni coppia di dati di training
fornisce un valore che rappresenta il prodotto scalare della coppia.

\subsubsection{Metodi parametrici per il kernel learning}\label{metodi-parametrici-per-il-kernel-learning}

L'idea è quella di prendere una funzione kernel come RBF e aggiungerci
dei parametri.

$$
k(x,z) = e^{-(x-z)^t M(x-z)}
$$

e se $M = \beta_0 I$ si ottiene la versione classica di RBF

$$
k(x,z) =e^{-\gamma||x-z||^2}
$$

Un'altra scelta tipica è $M = diag(\beta_1, \ldots,\beta_m)$, in questo
caso le distanze vengono pesate dando maggiore importanza rispetto ad
altre.

Con questo approccio vengono appresi anche i parametri $\gamma$.

\subsubsection{Transductive feature extraction con kernel non
lineari}\label{transduction-feature-extraction-con-kernel-non-lineari}

Effettua una feature extraction implicitamente nel feature space.

\textbf{KPCA} o Kernel Principal Component Analisys va a calcolare
implicitamente le proiezioni delle feature principali sugli autovettori
(direzioni) principali.

Trattandosi di un caso non lineare, non è possibile andare a calcolare
esplicitamente le componenti principali, ma viene utilizzata una
formula che permette di calcolarle in modo più agevole
(implicitamente), un po' come accade con i margini delle SVM.

Trattandosi di un approccio trasduttivo, può essere utilizzato
solo se si hanno ha disposizione tutte le componenti. Se questo non è possibile è necessario utilizzare delle
tecniche dette \textbf{out sample}.

\subsubsection{Spectral Kernel learning}\label{spectral-kernel-learning}

La matrice kernel, essendo definita positiva, può essere scritta in un
modo compatto in funzione degli autovalori e autovettori.

$$
K = \sum_{s=1}^{n} \lambda_s u_s \cdot u_s^t
$$

dove:
\begin{itemize}
	\item $\lambda_s$ sono gli autovalori di $K$
	\item $u_s$ sono gli autovettori di \textit{K}
\end{itemize}

Utilizzando una trasformazione degli autovalori si va ad agire
implicitamente nello spazio delle feature. 
Il kernel modificato può essere ottenuto utilizzando un po' di algebra, andando a
moltiplicare la matrice degli autovettori con la radice quadrata della
matrice degli autovalori, ovvero:

$$
X = \sqrt{U \Lambda}
$$

L'idea è quindi quella di ottimizzare lo spettro della matrice kernel, ovvero i suoi autovalori, in modo trasduttivo.

\subsubsection{Multiple kernel learning}\label{multiple-kernel-learning}

L'idea è quella di combinare linearmente kernel diversi e definiti a
priori, per poi apprendere la combinazione lineare di questi in modo da
ottimizzare il kernel risultante.

$$
K = \sum_{s=1}^n \mu_s K_s
$$

\begin{itemize}
\item
  \textbf{Fixed methods}: o metodi euristici, vengono utilizzati
  semplici regole per il calcolo dei coefficienti,
  ad esempio il metodo più semplice è prendere la media dei kernel e
  stranamente, a livello pratico, funziona molto bene, oppure possono
  essere scelti dei pesi in base all'accuratezza dei singoli kernel
  (funziona peggio rispetto alla media).
\item
  \textbf{Optimization method}: inglobano i coefficienti come variabile
  da apprendere nel problema di ottimizzazione, ad esempio nelle SVM si
  può modificare il problema di apprendimento per apprendere sia i
  coefficienti $\alpha$, sia per i coefficienti della combinazione lineare $\mu$.
\end{itemize}
