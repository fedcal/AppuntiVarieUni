#Lezione 14 - Funzioni Kernel 

![](./immagini/l14-kernel.png)

In pratica la funzione Kernel serve per calcolare un prodotto scalare in uno spazio a pi√π dimensioni.

##Rappresentazione dei dati con i Kernel

Le funzioni Kernel permettono di andare a definire una serie di metodi per l'apprendimento supervisionato.

Ad esempio data una serie di oggetti *S = {x<sub>1</sub>,x<sub>2</sub>,.., x<sub>n</sub>}* pu√≤ essere rappresentata con i Kernel come una funzione

> *k*: X *x* X -> R

Cio√® una funzione che confronta le varie coppie della serie e le valuta utilizzando un numero reale.

Il dataset *S* pu√≤ essere quindi rappresentato con una matrice simmetrica *K<sub>i,j</sub> = k(x<sub>i</sub>,x<sub>j</sub>)*. Inoltre, dal momento che la funzione *k* rappresenta un prodotto scalare su un certo spazio, la matrice *k* √® semi-definita positiva (questa realzione vale in se e solo se).

** aggiungi immagine cone definzione di matrice definita semi positva**

###Vantaggi di questa rappresentazione

La rappresentazione dei dati con matrici kernel ha come vantaggi:

- lo stesso algortimo pu√≤ essere utilizzato per analizzare dati diveri, quindi tutti gli algoritmi **kernel based** saranno definiti sulla forma della matrice.
- la progettazione dei kernel e degli algortimi √® modulare
- risulta pi√π semplice integrare viste diverse di oggetti, non sempre esiste una rappresentazione ottimale dello stesso oggetto, diventa quindi possibile combinare tra loro queste rappresentazioni.
- La dimensionalit√† dei dati dipente solo dal numero di oggetti e non dalla loro dimensione vettoriale.
- La comparazione tra oggetti pu√≤ risultare pi√π semplice rispetto ad una loro esplicita rappresentazione.

###Metodi Kernel

Molti metodi kernel, comprese le SVM possono essere interpretati come algortimi che, dato un insieme di oggetti *S* risolvono un problema di minimo di una certa funzione *L* associata al rischio empirico.

**immagine 1**

###Modularit√† dei metodi Kernel

I metodi Kernel possono essere rappresentatni da 5 fasi modulari

1. *n*-oggetti
2. definizione della funzione kernel
3. costruzione della matrice *K*
4. applicazione dell'algoritmo su *K* e *Y* (valori target attesi) (ad esempio SVM)
5. produzione dalla funzione

**sostituire con l'immagine**

##Kernel Trick

Ogni algoritmo per i dati vettoriali che pu√≤ essere espresso in termini del prodotto scalre tra vettori pu√≤ essere implicitamente eseguito nello spazio delle feature associatio ad un determinato kernel, rimpiazzando i prodotti scalari con valutazioni kernel.

1. Kerneliizzazione di metodi lineari o basati su distenze, come il Perceptron e K-NN.
2. Applicazione di algoritmi definiti su vettori a dati non vettoriali, utilizzando dei kernel definiti per dati non vettoriali.

Ad esempio K-NN pu√≤ utilizzare i kernel per calcolare la distanza tra due vettori.

##Tipologie di Kernel

Per **vettori** si possono utilizzare come kernel:

- **lineare**: *k(x,z) = x \* z*
- **polinomiale**: _k(x,z) = (x * z + c)<sup>d</sup>_
- **gaussiano** (RBF): _k(x,z) = exp(-ùú∏||x-z||<sup>2</sup>)_, ha la caratteristica di essere sempre compreso tra 0 e 1.

Il fatto che il kernel sia sempre maggiore di 0, implica che i due vettori sono nello stesso ottante (tra i due vettori c'√® un angolo minore di 90¬∞).

Se *k(x,x)* √® uguale a 1 si dice che il kernel √® **normalizzato**, ovvero tutti i vettori del feature space sono normalizzati. La matrice kernel definita con un kernel normalizzato ha tutti 1 nella diagonale.

√à sempre possibile normalizzare un kernel *k(x,z)* dividendolo per la radice quandrata di _k(x,x) * k(z,z)_

Come kernel per le **stringhe** si possono contare tutte le sequenze di una cerca lunghezza e costruire un vettore delle feature delle occorrenze, questo si fa con le tecniche di programmazione dinamica.

Per gli alberi si possono utilizzare delle tecniche analoghe, considerando i sotto alberi in comune.

C'√® un libro **Kernel Methods for Pattern Analysis** che spiega molto bene questa tecnica.

##Operazioni sui Kernel

Una combinazione lineare positiva di kernel √® anchessa un kernel, quindi

> k(x,z) = ak<sub>1</sub>(x,z) + bk<sub>2</sub>(x,z)

Se quna sequenza di kernel converge puntualmente ad una funzione *f*, allora anche *f* √® un kernel.

Ma c'√® di pi√π, i kernel possono essere tra loro composti per ottenere altri kernel.

Tutto questo si pu√≤ andare a combinare per ottenere un kernel migliore.
Dato un insieme *S* di kernel si pu√≤ definire

> k(x,z) = Sommatoria<sub>[S=1,Q]</sub>Œº<sub>s</sub>k<sub>s</sub>(x,z)

Dove Œº √® un vettore tale che la loro sommatoria sia 1.

L'idea del Multiple Kernel Learning √® di definire degli algoritmi per apprendere i valori di Œº<sub>s</sub> della combinazione che migliorino le prestazione di una SVM usando il kernel combinato, rispetto alle SVM ottenute utilizzando i kernel individuali.

