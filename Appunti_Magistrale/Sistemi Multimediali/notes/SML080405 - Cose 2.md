# Algoritmi specializzati per qualcosa

## RayTracing

Cerca di ricostruire in modo più corretto l'iterazione tra i raggi luminosi e l'occhio umano.

Si tratta di un algoritmo pesante che tipicamente viene utilizzato in post-produzione di immagini/filmati piuttosto che in ambito real time

## Local vs global rendering

Il rendering locale viene fatto sui singoli oggetti in modo indipendente, ovvero quello che abbiamo visto finora. 

RayTracing e Radiosity (l'altro algoritmo) funzionano a livello globale, conoscono l'interno mondo quando viene renderizzato la scena e quindi riescono ad effettuare cose più complicate come le superfici riflettinti/trasparenti e le ombre.

Con RayTracing non viene determianto il colore di un oggetto, ma del singolo pixel, tenendo in considerazione tutto il mondo che si sta realizzando.

**Backward RayTracing**: traccia i raggi dal centro di proiezioni (punto camera) verso ogni singolo pixel del paino immagine.
Tracciando il raggio si riesce a raggiungere la parte dell'oggetto colpito, dalla quale è poi possibile risalire alla sorgente luminosa che lo colpisce.
Così facendo è possibile tenere in considerazione anche la tipologia del materiale dell'oggetto e determianre se la parte dell'oggetto è in ombra oppure se è colpita da una luce riflessa.

C'è un che di ricorsivo nella determinazione della luce che colpisce la superfice dell'oggetto, quinidi tipicamente viene posto un limite al numero di iterazioni ricorsive.

(Si ottengono degli effetti fighi, anche quando ci sono delle ombre semitrasparenti)

Tra le varie cose difficili che fa questo algoritmo c'è il calcolo dell'intersezione tra il raggio di luce e la superfice, vengono quindi utilizzate delle tecniche di analisi specializzate per le varie tipologie di superfice come sfere, piani, poligoni, "quadratics".

## Radiosity

Altro algoritmo per la gestione della luce, anche gli oggetti emanano della luce (?).

Per definire il colore di un oggetto è necessario risolvere un sistema lineare, perché dipende sia dalla sorgente luminonsa che da **tutti** gli altri oggetti della scena.

Il fotorealismo che si ottiene con questo algoritmo è estremo, è lo stesso che viene usato nella CGI nei film, il problema è che non si riesce ancora ad applicarlo in real time.

--- 

FINE PARTE GRAFICA

INIZIO PARTE "Gestione del suono"

---

L'aria è un fluido, cose, come funziona l'orecchio.

L'orecchio umano riesce a sentire le variazioni di pressione che vanno da 20Hz a 20KHz.

La sensibilità massima è tra i 2 e 4KHz, mentre l'intensità sonora necessaria necessaria per sentire un suono deve essere più alta se il suono ha una frequenza che si allontana dall'invervallo ottimo.

L'intensità sonora si misura in Decibel (dB), una scala di misura relativa che ha bisogno di un punto di riferimento.

# Come funziona la digitalizzazione di un suono. 

C'è una membrana che converte le onde sonore creando delle variazione di tensione che vengono registrate da un sensore che digitilizza il segnale, campionandolo ogni tot tempo.

Si ha quindi che il segnale sonoro viene prima trasformato in un segnale elettrico e poi campionato e rappresentato con dei bit.

Ci sono un po' di sfighe, il campionamento di per se non è preciso, in più nella trasformazione da analogico a digitale c'è il rischio che si verifichi un errore di quantizzazione per via del numero di bit limitato per rappresentare l'intensità del segnale.

Torna il teroma di Nyquist: la frequenza di campionamento deve essere doppia rispetto alla massima frequenza del segnale da campionare o della frequenza d'interesse.

Quindi la nostra frequenza di campionamento deve essere di circa 40KHz, anche se poi nel lato pratico si campiona ancora più frequentemente (44,1KHz).

L'intensità sonora che ci interessa è quella di 60dB, quindi tipicamente vengono utilizzati 16 bit, anche se negli studi di registrazione vengono utilizzati 24 o 32 bit (float).

L'errore di quantizzazione può essere visto come un rumore bianco (random) al sengale di partenza. E' importante che questo rumore sia sufficientemente piccolo da non essere udibile.

# LifeCycle del suono digitalizzato

1. Filtraggio (antialiasing)
2. Digitalizzazione (acquisizioni)
3. Filtraggio (postproduzione, correzioni, miglioramenti)
4. Memorizzazione (compressione)
5. Riproduzione (decompressione/filtraggio)
6. Formati standard di distribuzione

C'è anche l'audio sintetizzato (generazione e locazione nello spazio, effetto Doppler) e la sintesi vocale (text2speech).

Per riprodurre il segnale campionato basta un filtro passa basso che smussa il segnale digitale che riceve dall'integrato.

**anti-aliasing**: filtro che toglie le scalettature sulle figure renderizzate a video, tuttavia è nato prima per il mondo audio nella ricostruzione del segnale, trasformandolo da scalettato in un filtro morbido.

Quando ci sono dei picchi a delle frequenze ben precise e sparse il nostro cervello lo interpreta come se fosse del rumore.

Un suono invece è caratterizzato da una forma d'onda con una certa periodicità.

# Come possiamo maltrattare l'audio

Una volta digitalizzato l'audio può essere elaborato per alterarlo oppure per comprimerlo.

Gli algoritmi di compressione lossless funzionano male perché le forme d'onda che si ripetono non sono sempre uguali ma ci sono delle differenze minime che rendono inutili gli algoritmi che tentano di eliminare le ridondanze esatte.

La trasformata di Fourier viene utilizzata solo come strumento di analisi e non di filtraggio perché tipicamente è necessario intervenire in tempo reale sul segnale.

Kernel fisso con streaming audio che modifica in tempo reale il segnale con un filtro a convoluzione.
